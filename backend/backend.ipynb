{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG Digitizer — Colab Inference Backend\n\n> Convert a scanned or photographed 12-lead ECG into structured time-series data.  \n> Upload an image → get back 12 calibrated voltage traces in mV, ready for analysis or EHR export.\n\n---\n\n## How it works\n\nThe pipeline is built on top of [hengck23's PhysioNet 2024 solution](https://www.kaggle.com/code/hengck23/demo-submission), extended with a serving layer and a lead-localization module. Three networks run in sequence:\n\n```\nInput image\n    │\n    ▼\n[Stage 0]  ResNet keypoint detector\n           Finds the four corners of the ECG paper → homography warp\n    │\n    ▼\n[Stage 1]  Grid alignment network\n           Detects the mm-grid intersections → rectifies any remaining distortion\n    │\n    ▼\n[Stage 2]  ResNet34-UNet (Net3)\n           Pixel-level segmentation of 4 simultaneous waveform rows\n           → peak tracking → mV conversion → Savitzky-Golay smoothing\n    │\n    ▼\n12-lead signals  +  per-lead bounding boxes  +  step-by-step images\n```\n\nThe key insight from the original competition solution: rather than detecting individual leads separately, Stage 2 predicts **4 probability maps simultaneously** — one per printed row. Each row contains 4 leads side by side, so a single forward pass recovers all 12 traces.\n\n---\n\n## Before you start\n\n1. **GPU runtime** — Runtime → Change runtime type → T4 GPU (or A100 if available)\n2. **`kaggle.json`** — download from kaggle.com → Settings → API → Create New Token\n3. **ngrok authtoken** — free account at ngrok.com → Dashboard → Your Authtoken\n\nRun the cells top to bottom. The last cell starts the server and prints the public URL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 — Install dependencies\n\n`connected-components-3d` is needed by the Stage 2 post-processing step to trace waveform centerlines.  \nEverything else is standard inference stack.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q fastapi uvicorn pyngrok python-multipart nest-asyncio\n!pip install -q timm connected-components-3d kaggle\nprint('✅ done')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 — Download model weights\n\nThe three checkpoint files (~400 MB total) are hosted as a Kaggle dataset that mirrors hengck23's original submission.  \nUpload your `kaggle.json` when prompted — Colab will save it to `/root/.kaggle/` and set the right permissions.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\nfrom google.colab import files\n\nprint('Upload kaggle.json  (kaggle.com → Settings → API → Create New Token)')\nfiles.upload()\n\nos.makedirs('/root/.kaggle', exist_ok=True)\n!cp kaggle.json /root/.kaggle/kaggle.json\n!chmod 600 /root/.kaggle/kaggle.json\nprint('✅ Kaggle API configured')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "WEIGHTS_DIR = '/content/weights'\nos.makedirs(WEIGHTS_DIR, exist_ok=True)\n\nprint(' Downloading weights (~5–10 min)...')\n!kaggle datasets download -d kami1976/hengck23-submit-physionet -p {WEIGHTS_DIR} --unzip -q\n\nHENGCK_DIR = f'{WEIGHTS_DIR}/hengck23-submit-physionet'\nWEIGHT_DIR = f'{HENGCK_DIR}/weight'\n\nprint('\\nVerifying:')\nfor fname in [\n    'stage0-last.checkpoint.pth',\n    'stage1-last.checkpoint.pth',\n    'stage2-00005810.checkpoint.pth',\n    '../stage0_common.py',\n]:\n    path = f'{WEIGHT_DIR}/{fname}'\n    ok = os.path.exists(path)\n    print(f'  {\"✅\" if ok else \"❌\"}  {fname}')\n\nprint('\\n✅ All files present — proceed to Cell 3' if os.path.exists(f'{HENGCK_DIR}/stage0_common.py')\n      else '\\n❌ stage0_common.py missing — download likely failed, re-run this cell')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 — Configure paths and imports\n\nAll weight paths are set once here and reused across later cells.  \n`sys.path` is updated so the helper modules bundled inside the Kaggle dataset (`stage0_common.py`, etc.) are importable.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys, os, gc, cv2, base64\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nimport timm\nfrom scipy.signal import savgol_filter\n\nWEIGHTS_DIR = '/content/weights'\nHENGCK_DIR  = f'{WEIGHTS_DIR}/hengck23-submit-physionet'\nWEIGHT_DIR  = f'{HENGCK_DIR}/weight'\n\nSTAGE0_W = f'{WEIGHT_DIR}/stage0-last.checkpoint.pth'\nSTAGE1_W = f'{WEIGHT_DIR}/stage1-last.checkpoint.pth'\nSTAGE2_W = f'{WEIGHT_DIR}/stage2-00005810.checkpoint.pth'\n\nif HENGCK_DIR not in sys.path:\n    sys.path.insert(0, HENGCK_DIR)\n\nimport stage0_common as s0c\nimport stage1_common as s1c\nimport stage2_common as s2c\nfrom stage0_model import Net as Stage0Net\nfrom stage1_model import Net as Stage1Net\nfrom stage2_model import MyCoordUnetDecoder, encode_with_resnet\n\nLEADS_ORDER = ['I','II','III','aVR','aVL','aVF','V1','V2','V3','V4','V5','V6']\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'device: {device}')\n\n# quick sanity check\nfor label, path in [('stage0 weight', STAGE0_W), ('stage1 weight', STAGE1_W), ('stage2 weight', STAGE2_W)]:\n    assert os.path.exists(path), f'Missing: {path}'\nprint('✅ paths OK')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 — Image preprocessing utilities\n\nReal-world ECG photos come from a wide range of sources — clinical scanners, phone cameras, aging paper — so a one-size-fits-all preprocessing step doesn't work well. This section defines a small toolkit of corrections and a dispatch function (`preprocess_by_source`) that picks the right combination based on the image origin.\n\n**CLAHE on the V channel** is the most universally useful step. Instead of stretching the global histogram, it operates on small tiles independently, which recovers local contrast on faded or unevenly lit paper without blowing out already-bright regions.\n\n`stage1_quality` gives a cheap edge-density + anisotropy score used later to decide which of two preprocessing paths produced the cleaner input for Stage 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Contrast / color corrections ──────────────────────────────────────────────\n\ndef change_color(image_rgb):\n    \"\"\"CLAHE on the HSV V channel — the core contrast enhancement used before Stage 0.\"\"\"\n    hsv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)\n    h, s, v = cv2.split(hsv)\n    v_denoised = cv2.fastNlMeansDenoising(v, h=5.46)\n    clip_limit = max(1.0, min(3.5, 2.0 + np.std(v_denoised) / 25))\n    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(8, 8))\n    return cv2.cvtColor(cv2.merge([h, s, clahe.apply(v_denoised)]), cv2.COLOR_HSV2RGB)\n\ndef clahe_luminance_bgr(img_bgr, clip=2.0, tile=8):\n    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n    l, a, b = cv2.split(lab)\n    clahe = cv2.createCLAHE(clipLimit=float(clip), tileGridSize=(int(tile), int(tile)))\n    return cv2.cvtColor(cv2.merge([clahe.apply(l), a, b]), cv2.COLOR_LAB2BGR)\n\ndef grayworld_white_balance(img_bgr):\n    img = img_bgr.astype(np.float32)\n    b, g, r = cv2.split(img)\n    m = (b.mean() + g.mean() + r.mean()) / 3.0\n    b *= m / (b.mean() + 1e-6)\n    g *= m / (g.mean() + 1e-6)\n    r *= m / (r.mean() + 1e-6)\n    return np.clip(cv2.merge([b, g, r]), 0, 255).astype(np.uint8)\n\ndef denoise_median(img_bgr, k=3):\n    k = int(k); k = k if k % 2 == 1 else k + 1\n    return cv2.medianBlur(img_bgr, k)\n\ndef denoise_bilateral(img_bgr, d=7, sigmaColor=50, sigmaSpace=50):\n    return cv2.bilateralFilter(img_bgr, int(d), float(sigmaColor), float(sigmaSpace))\n\ndef illumination_strength(img_bgr, sigma=35):\n    \"\"\"Estimate global illumination unevenness via low-frequency std.\"\"\"\n    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0\n    return float(np.std(cv2.GaussianBlur(gray, (0, 0), sigma)))\n\ndef bg_correct_lab_l(img_bgr, k=81):\n    \"\"\"Subtract a morphological background estimate from the L channel.\"\"\"\n    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n    l, a, b = cv2.split(lab)\n    k = int(k); k = k if k % 2 == 1 else k + 1\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))\n    bg = cv2.morphologyEx(l, cv2.MORPH_OPEN, kernel)\n    l_corr = cv2.normalize(cv2.subtract(l, bg), None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n    return cv2.cvtColor(cv2.merge([l_corr, a, b]), cv2.COLOR_LAB2BGR)\n\n\n# ── Source-aware dispatch ──────────────────────────────────────────────────────\n# Source codes map to equipment types in the PhysioNet dataset.\n# When no classifier is available, '0001' (identity) is the safe default.\n\ndef preprocess_by_source(img_bgr, source='0001'):\n    s = str(source)\n    if s in ('0001', '0004', '0012'):\n        return img_bgr\n    if s in ('0003', '0011'):\n        return clahe_luminance_bgr(grayworld_white_balance(img_bgr), clip=1.2)\n    if s == '0006':\n        return clahe_luminance_bgr(denoise_bilateral(img_bgr, d=5, sigmaColor=25, sigmaSpace=25), clip=1.2)\n    if s in ('0005', '0010'):\n        x = img_bgr\n        if illumination_strength(x) > 0.14:\n            x = bg_correct_lab_l(x, k=81)\n        if cv2.cvtColor(x, cv2.COLOR_BGR2GRAY).std() < 30:\n            x = clahe_luminance_bgr(x, clip=1.1)\n        return x\n    if s == '0009':\n        x = img_bgr\n        if illumination_strength(x) > 0.14:\n            x = bg_correct_lab_l(x, k=101)\n        return denoise_median(x, k=3)\n    return img_bgr\n\n\n# ── Post-pipeline helpers ──────────────────────────────────────────────────────\n\ndef stage1_quality(s1_rgb):\n    \"\"\"\n    Score used to pick the better of two preprocessing branches.\n    Combines Canny edge density (favors sharp waveforms) with a horizontal/vertical\n    gradient anisotropy term (favors well-aligned grids).\n    \"\"\"\n    g = cv2.cvtColor(s1_rgb.astype(np.uint8), cv2.COLOR_RGB2GRAY)\n    density = cv2.Canny(g, 50, 150).mean() / 255.0\n    gx = cv2.Sobel(g, cv2.CV_32F, 1, 0, ksize=3)\n    gy = cv2.Sobel(g, cv2.CV_32F, 0, 1, ksize=3)\n    ax, ay = float(np.mean(np.abs(gx))), float(np.mean(np.abs(gy)))\n    anis = max(ax, ay) / (min(ax, ay) + 1e-6)\n    return float(density * 0.7 + np.tanh(anis - 1.0) * 0.3)\n\ndef series_dict(series_4row):\n    \"\"\"\n    Unpack the (4, N) output of Stage 2 into a named dict.\n\n    The ECG is printed in 3 data rows × 4 columns + 1 full-length rhythm strip (row 3 = Lead II).\n    Each data row gets split into 4 equal segments, then named by standard 12-lead convention.\n    \"\"\"\n    series_4row = np.asarray(series_4row)\n    if series_4row.ndim == 3:\n        series_4row = series_4row[0]\n    if series_4row.shape[0] != 4 and series_4row.shape[1] == 4:\n        series_4row = series_4row.T\n    d = {}\n    names = [['I', 'aVR', 'V1', 'V4'], ['II_short', 'aVL', 'V2', 'V5'], ['III', 'aVF', 'V3', 'V6']]\n    for r in range(3):\n        for lead, arr in zip(names[r], np.array_split(series_4row[r], 4)):\n            d[lead] = np.asarray(arr, dtype=np.float32)\n    d['II'] = np.asarray(series_4row[3], dtype=np.float32)  # full-length rhythm strip\n    return d\n\ndef dw(d, alpha=0.33):\n    \"\"\"\n    Soft Einthoven correction.\n    II = I + III theoretically, but digitization errors accumulate in each channel independently.\n    This blends the residual error back proportionally rather than forcing an exact constraint.\n    \"\"\"\n    if all(k in d for k in ['I', 'II_short', 'III']):\n        L1, L2s, L3 = d['I'], d['II_short'], d['III']\n        e = L2s - (L1 + L3)\n        d['I']         = L1  + alpha * e\n        d['III']       = L3  + alpha * e\n        d['II_short']  = L2s - alpha * e\n    return d\n\ndef img_to_b64(img_rgb: np.ndarray, max_width=1200) -> str:\n    h, w = img_rgb.shape[:2]\n    if w > max_width:\n        img_rgb = cv2.resize(img_rgb, (max_width, int(h * max_width / w)))\n    _, buf = cv2.imencode('.jpg', cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR),\n                          [cv2.IMWRITE_JPEG_QUALITY, 88])\n    return 'data:image/jpeg;base64,' + base64.b64encode(buf).decode()\n\ndef heatmap_overlay(img_rgb: np.ndarray, pixel_4ch: np.ndarray) -> str:\n    \"\"\"Max-pool the 4 activation channels and overlay as a JET colormap.\"\"\"\n    h, w = img_rgb.shape[:2]\n    heat = pixel_4ch.max(axis=0)\n    heat = cv2.resize(heat, (w, h))\n    heat = np.uint8(255 * heat / (heat.max() + 1e-6))\n    heat_color = cv2.cvtColor(cv2.applyColorMap(heat, cv2.COLORMAP_JET), cv2.COLOR_BGR2RGB)\n    overlay = np.uint8(img_rgb * 0.55 + heat_color * 0.45)\n    return img_to_b64(overlay)\n\nprint('✅ utilities defined')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 — Net3: the waveform segmentation model\n\nNet3 is a standard ResNet34 encoder paired with a UNet-style decoder, with one small but important addition: a **coordinate channel** appended to the final feature map before the output convolution.\n\nThe coordinate channel is a vertical gradient (0 at top → 1 at bottom) broadcast across the full width. This gives the model an explicit signal about vertical position, which matters because the four ECG rows have different absolute y-positions in the image. Without it, the model would need to infer row identity purely from context.\n\nThe output is 4 channels — one probability map per printed row — not 12. The lead splitting happens algebraically afterward in `series_dict`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Net3(nn.Module):\n    def __init__(self, pretrained=False):\n        super().__init__()\n        encoder_dim = [64, 128, 256, 512]\n        decoder_dim = [256, 128, 64, 32]\n        self.encoder = timm.create_model(\n            'resnet34.a3_in1k', pretrained=pretrained,\n            in_chans=3, num_classes=0, global_pool=''\n        )\n        self.decoder = MyCoordUnetDecoder(\n            in_channel=encoder_dim[-1],\n            skip_channel=encoder_dim[:-1][::-1] + [0],\n            out_channel=decoder_dim,\n            scale=[2, 2, 2, 2],\n        )\n        self.pixel = nn.Conv2d(decoder_dim[-1] + 1, 4, 1)  # +1 for the coord channel\n\n    def forward(self, image):\n        encode = encode_with_resnet(self.encoder, image)\n        last, _ = self.decoder(feature=encode[-1], skip=encode[:-1][::-1] + [None])\n        B, C, H, W = last.shape\n        # vertical coordinate channel: shape (B, 1, H, W), values in [0, 1]\n        coord = torch.linspace(0, 1, H, device=last.device).view(1, 1, H, 1).expand(B, 1, H, W)\n        last = torch.cat([last, coord], dim=1)\n        return self.pixel(last)\n\nprint('✅ Net3 defined')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 — PhysioPipeline\n\nThe pipeline class wires the three stages together and handles the calibration arithmetic.\n\n**Calibration constants** (`zero_mv`, `mv_to_pixel`, `t0`/`t1`) come from the physical properties of standard ECG paper:\n- Paper speed: 25 mm/s → at the model's internal resolution of 4352 px wide and ~10 s of signal, each second is ~392 px\n- Amplitude scale: 10 mm/mV → `mv_to_pixel = 78.8` (measured empirically from the dataset)\n- `zero_mv` lists the y-pixel of each row's isoelectric baseline in the 4352×1696 resized space\n\n**Dual-path preprocessing**: Stage 0 and Stage 1 run twice — once on the raw image and once on the preprocessed version — and `stage1_quality` picks the winner. This costs an extra few seconds but noticeably improves robustness on difficult inputs (e.g. strong background illumination gradients).\n\n**`_compute_lead_boxes`** works backwards from these same calibration constants to compute where each of the 12 leads sits in the image, without any additional inference. The x-boundaries come from `t0`/`t1` divided into 4 equal columns; the y-boundaries are derived from `zero_mv` row gaps. The result is normalized to [0, 1] so the frontend can map them onto any display size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6b — Clinical metrics\n\nComputed server-side from the Lead II rhythm strip so the frontend doesn't need to implement peak detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scipy.signal import find_peaks\n\ndef compute_metrics(lead_ii: list, fs: int = 500) -> dict:\n    \"\"\"\n    Compute HR, RR interval, and QRS duration from the Lead II rhythm strip.\n\n    Uses scipy.signal.find_peaks with physiologically-grounded constraints:\n      - minimum peak distance: 250 samples (= 0.5 s, equivalent to 120 bpm ceiling)\n      - adaptive height threshold: 60th percentile of absolute signal values,\n        so the threshold scales with whatever amplitude the digitizer produced\n\n    RR and HR use the median rather than the mean to be robust against the\n    occasional missed or spurious peak at the start/end of the strip.\n\n    QRS duration is estimated by walking outward from each R peak until the\n    signal drops below 10% of that peak's amplitude, averaging across all beats.\n    \"\"\"\n    sig = np.array(lead_ii, dtype=np.float32)\n    if len(sig) < fs:\n        return {'hr_bpm': None, 'rr_ms': None, 'qrs_ms': None}\n\n    # Adaptive height threshold — scales with signal amplitude\n    height_thresh = np.percentile(np.abs(sig), 60)\n\n    peaks, _ = find_peaks(\n        sig,\n        distance=int(fs * 0.5),   # no two R-peaks closer than 0.5 s\n        height=height_thresh,\n    )\n\n    if len(peaks) < 2:\n        return {'hr_bpm': None, 'rr_ms': None, 'qrs_ms': None}\n\n    rr_samples = np.diff(peaks)\n    rr_ms      = float(np.median(rr_samples) / fs * 1000)\n    hr_bpm     = round(60_000 / rr_ms, 1)\n\n    # QRS duration: walk left and right from each peak until signal < 10% of peak\n    qrs_durations = []\n    for pk in peaks:\n        amp = sig[pk]\n        threshold = amp * 0.10\n\n        # walk left\n        left = pk\n        while left > 0 and sig[left] > threshold:\n            left -= 1\n\n        # walk right\n        right = pk\n        while right < len(sig) - 1 and sig[right] > threshold:\n            right += 1\n\n        width_ms = (right - left) / fs * 1000\n        # sanity bounds: 40–200 ms is the physiological range\n        if 40 <= width_ms <= 200:\n            qrs_durations.append(width_ms)\n\n    qrs_ms = round(float(np.median(qrs_durations)), 1) if qrs_durations else None\n\n    return {\n        'hr_bpm': hr_bpm,\n        'rr_ms':  round(rr_ms, 1),\n        'qrs_ms': qrs_ms,\n    }\n\nprint('✅ compute_metrics defined')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class PhysioPipeline:\n    # Standard 12-lead print layout: 3 data rows × 4 columns\n    # Row 3 is the full-length Lead II rhythm strip\n    LEAD_LAYOUT = [\n        ('I',   0, 0), ('aVR', 0, 1), ('V1', 0, 2), ('V4', 0, 3),\n        ('II',  1, 0), ('aVL', 1, 1), ('V2', 1, 2), ('V5', 1, 3),\n        ('III', 2, 0), ('aVF', 2, 1), ('V3', 2, 2), ('V6', 2, 3),\n    ]\n\n    def __init__(self, device='cuda:0'):\n        self.device = device\n        self.stage0_net = self.stage1_net = self.stage2_net = None\n\n        # Stage2 reads this ROI from the Stage1 output\n        self.x0, self.x1 = 0, 2176\n        self.y0, self.y1 = 0, 1696\n\n        # Calibration: isoelectric baseline y-positions in the 4352×1696 resized space\n        # and the pixel-per-millivolt conversion factor\n        self.zero_mv     = [703.5, 987.5, 1271.5, 1531.5]\n        self.mv_to_pixel = 78.8\n\n        # Usable signal x-range in the resized space (trims left/right borders and scale bars)\n        self.t0, self.t1 = 235, 4161\n\n        self.resize = T.Resize((1696, 4352), interpolation=T.InterpolationMode.BILINEAR)\n\n    # ── Model loading ──────────────────────────────────────────────────────────\n\n    def load_models(self, stage0_w, stage1_w, stage2_w):\n        print('  Stage0 (rotation correction)...')\n        self.stage0_net = s0c.load_net(Stage0Net(pretrained=False), stage0_w).to(self.device).eval()\n        print('  Stage1 (grid alignment)...')\n        self.stage1_net = s1c.load_net(Stage1Net(pretrained=False), stage1_w).to(self.device).eval()\n        print('  Stage2 (Net3 waveform segmentation)...')\n        self.stage2_net = Net3(pretrained=False).to(self.device).eval()\n        st = torch.load(stage2_w, map_location='cpu')\n        if isinstance(st, dict) and 'state_dict' in st:\n            st = st['state_dict']\n        self.stage2_net.load_state_dict(st, strict=False)\n        print('✅ all models loaded')\n\n    # ── Stage inference ────────────────────────────────────────────────────────\n\n    def _run_stage0(self, img_bgr):\n        try:\n            img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n            batch   = s0c.image_to_batch(change_color(img_rgb))\n            with torch.no_grad(), torch.amp.autocast(self.device.split(':')[0], dtype=torch.float32):\n                output = self.stage0_net(batch)\n            rotated, keypoint = s0c.output_to_predict(img_rgb, batch, output)\n            keypoint = keypoint.astype(np.float32) if hasattr(keypoint, 'astype') else keypoint\n            normalised, _, _ = s0c.normalise_by_homography(rotated, keypoint)\n            return normalised\n        except Exception as e:\n            print(f'  ⚠️  Stage0 failed ({e}), falling back to resized original')\n            img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n            return cv2.resize(img_rgb, (2176, 1696))\n\n    def _run_stage1(self, stage0_rgb):\n        batch = {'image': torch.from_numpy(\n            np.ascontiguousarray(stage0_rgb.transpose(2, 0, 1))).unsqueeze(0)}\n        with torch.no_grad(), torch.amp.autocast(self.device.split(':')[0], dtype=torch.float32):\n            output = self.stage1_net(batch)\n        gridpoint_xy, _ = s1c.output_to_predict(stage0_rgb, batch, output)\n        return s1c.rectify_image(stage0_rgb, gridpoint_xy)\n\n    def _run_stage2(self, stage1_rgb, length):\n        img   = stage1_rgb[self.y0:self.y1, self.x0:self.x1] / 255.0\n        batch = self.resize(\n            torch.from_numpy(np.ascontiguousarray(img.transpose(2, 0, 1))).unsqueeze(0)\n        ).float().to(self.device)\n        with torch.no_grad(), torch.amp.autocast(self.device.split(':')[0], dtype=torch.float32):\n            output = self.stage2_net(batch)\n        pixel = torch.sigmoid(output).float().cpu().numpy()[0]   # (4, H, W)\n        # pixel_to_series traces the waveform centerline in each row\n        series_px = s2c.pixel_to_series(pixel[..., self.t0:self.t1], self.zero_mv, length)\n        # convert pixel displacement from baseline → millivolts\n        series = (np.array(self.zero_mv).reshape(4, 1) - series_px) / self.mv_to_pixel\n        for i in range(4):\n            series[i] = savgol_filter(series[i], window_length=7, polyorder=2)\n        return series, pixel\n\n    # ── Lead bounding boxes ────────────────────────────────────────────────────\n\n    def _compute_lead_boxes(self, img_h, img_w):\n        \"\"\"\n        Derive normalized [0,1] bounding boxes for all 12 leads using the same\n        calibration constants that Stage2 uses for extraction — no extra inference.\n\n        x-boundaries: divide the signal x-range [t0, t1] into 4 equal columns,\n                      then scale from the internal 4352-px width to the actual image width.\n        y-boundaries: divide the 3-row data area into equal thirds rather than using\n                      midpoints between baselines. Using midpoints fails when R-peaks\n                      extend beyond the halfway mark between rows, causing boxes to\n                      visually clip into adjacent lead regions.\n        \"\"\"\n        W_RS = 4352  # Stage2 internal width\n        sx   = img_w / W_RS\n\n        col_w   = (self.t1 - self.t0) / 4.0\n        x_edges = [(self.t0 + i * col_w) * sx for i in range(5)]\n\n        zm      = self.zero_mv\n        row_gap = zm[1] - zm[0]\n\n        # Define the usable data area top and bottom, then divide into 3 equal rows.\n        # This guarantees no overlap regardless of how far waveforms extend from baseline.\n        data_top = zm[0] - row_gap * 0.85   # clear the patient-info header\n        data_bot = zm[2] + row_gap * 0.55   # some padding below the last data row\n        row_h    = (data_bot - data_top) / 3.0\n\n        y_tops = [data_top + r * row_h for r in range(3)] + [(zm[2] + zm[3]) / 2.0]\n        y_bots = [data_top + (r + 1) * row_h for r in range(3)] + [float(img_h)]\n\n        boxes = {}\n        for lead, row, col in self.LEAD_LAYOUT:\n            boxes[lead] = {\n                'x1': round(max(0.0, x_edges[col]     / img_w), 4),\n                'y1': round(max(0.0, y_tops[row]       / img_h), 4),\n                'x2': round(min(1.0, x_edges[col + 1] / img_w), 4),\n                'y2': round(min(1.0, y_bots[row]       / img_h), 4),\n            }\n        return boxes\n\n    # ── Signal overlay ────────────────────────────────────────────────────────\n\n    def _draw_overlay(self, s1_rgb, series_4row, pixel_4ch):\n        \"\"\"\n        Draw the 12 extracted lead signals back onto the Stage 1 image.\n\n        Three-layer noise handling:\n          1. Percentile clip (2nd–98th) — robust outlier removal unaffected by extreme values\n          2. Median filter — removes spike artifacts without smearing edges like mean smoothing\n          3. Confidence masking — uses Stage 2 activation maps to skip low-confidence regions\n             rather than drawing a noisy trace that would mislead the viewer\n        \"\"\"\n        from scipy.signal import medfilt\n\n        img   = s1_rgb.copy().astype(np.uint8)\n        img_h, img_w = img.shape[:2]\n\n        sx    = img_w / 4352.0\n        col_w = (self.t1 - self.t0) / 4.0\n        zm    = self.zero_mv\n        series = np.asarray(series_4row)\n\n        # Row y-boundaries (equal thirds, same as _compute_lead_boxes)\n        row_gap  = zm[1] - zm[0]\n        data_top = zm[0] - row_gap * 0.85\n        data_bot = (zm[2] + zm[3]) / 2.0\n        row_h    = (data_bot - data_top) / 3.0\n        row_y_top = [int(data_top + r * row_h) for r in range(3)]\n        row_y_bot = [int(data_top + (r+1) * row_h) for r in range(3)]\n        strip_top = int((zm[2] + zm[3]) / 2.0)\n\n        COLORS = [\n            [(255,59,48),  (255,149,0),  (255,204,0),  (48,209,88)],\n            [(0,199,190),  (0,122,255),  (88,86,214),  (175,82,222)],\n            [(255,45,85),  (255,107,107),(50,173,230),  (76,217,100)],\n        ]\n\n        # Pre-compute per-column confidence from Stage 2 activation maps\n        # pixel_4ch shape: (4, H, W) in the 4352-wide resized space\n        # Resize activation to original image width for column slicing\n        # Confidence check: slice BOTH x (column range) AND y (row band) before scoring.\n        # Without the y slice, waveform pixels (<0.2% of full-height column) are buried\n        # in background, making any percentile/mean useless.\n        CONF_THRESHOLD = 0.25   # max activation in the row band below this → skip\n\n        act_col_w = (self.t1 - self.t0) / 4.0\n\n        # pixel_4ch is in 4352-wide space; height matches s1_rgb (not resized)\n        act_h = pixel_4ch.shape[1]   # typically 1696\n\n        def col_confidence(row_idx, col):\n            \"\"\"Max activation inside the correct row band and column range.\"\"\"\n            x_start = int(self.t0 + col * act_col_w)\n            x_end   = int(self.t0 + (col + 1) * act_col_w)\n            # y band: ±1.5 × half-row-gap around this row's baseline\n            half = int(row_gap * 0.75)\n            y_center = int(zm[row_idx])\n            y_start  = max(0, y_center - half)\n            y_end    = min(act_h, y_center + half)\n            region   = pixel_4ch[row_idx, y_start:y_end, x_start:x_end]\n            return float(np.max(region)) if region.size else 0.0\n\n        # Skip this many samples at the start of each column — covers the 1mV\n        # calibration pulse printed at the left edge of each lead region.\n        CALIB_SKIP = 80\n\n        def clean(sig):\n            \"\"\"\n            Two-pass outlier removal + median filter.\n            Pass 1: 5–95th percentile clip (tighter than before).\n            Pass 2: ±2×IQR around median to catch residual spikes that survive pass 1.\n            \"\"\"\n            s = np.array(sig, dtype=np.float32)\n            if len(s) < 5:\n                return s\n            lo, hi = np.percentile(s, 5), np.percentile(s, 95)\n            s = np.clip(s, lo, hi)\n            q25, q75 = np.percentile(s, 25), np.percentile(s, 75)\n            iqr = q75 - q25\n            if iqr > 0:\n                med = np.median(s)\n                s = np.clip(s, med - 2 * iqr, med + 2 * iqr)\n            k = min(15, len(s) if len(s) % 2 == 1 else len(s) - 1)\n            k = max(k, 5)\n            return medfilt(s, kernel_size=k)\n\n        for row in range(3):\n            row_signal = series[row]\n            seg_len    = len(row_signal) // 4\n            y_min, y_max = row_y_top[row], row_y_bot[row]\n            for col in range(4):\n                if col_confidence(row, col) < CONF_THRESHOLD:\n                    continue\n\n                seg   = clean(row_signal[col * seg_len : (col + 1) * seg_len])\n                color = COLORS[row][col]\n                x0_rs = self.t0 + col * col_w\n                pts   = []\n                for i, mv in enumerate(seg):\n                    if i < CALIB_SKIP:\n                        continue\n                    x_px = int((x0_rs + i / max(len(seg)-1, 1) * col_w) * sx)\n                    y_px = int(zm[row] - mv * self.mv_to_pixel)\n                    y_px = max(y_min, min(y_max, y_px))\n                    pts.append((x_px, y_px))\n                for k in range(1, len(pts)):\n                    cv2.line(img, pts[k-1], pts[k], color, thickness=2, lineType=cv2.LINE_AA)\n\n        # Lead II rhythm strip — use row 3 activation for confidence\n        rhythm_act      = pixel_4ch[3] if pixel_4ch.shape[0] > 3 else pixel_4ch[2]\n        rhythm_y_center = int(zm[3])\n        rhythm_y_start  = max(0, rhythm_y_center - int(row_gap * 0.75))\n        rhythm_y_end    = min(act_h, rhythm_y_center + int(row_gap * 0.75))\n        rhythm_region   = rhythm_act[rhythm_y_start:rhythm_y_end, self.t0:self.t1]\n        rhythm_conf     = float(np.max(rhythm_region)) if rhythm_region.size else 0.0\n        if rhythm_conf >= CONF_THRESHOLD:\n            rhythm = clean(series[3])\n            n_r    = len(rhythm)\n            pts    = []\n            for i, mv in enumerate(rhythm):\n                if i < CALIB_SKIP:\n                    continue\n                x_px = int((self.t0 + i / max(n_r-1, 1) * (self.t1 - self.t0)) * sx)\n                y_px = int(zm[3] - mv * self.mv_to_pixel)\n                y_px = max(strip_top, min(img_h - 1, y_px))\n                pts.append((x_px, y_px))\n            for k in range(1, len(pts)):\n                cv2.line(img, pts[k-1], pts[k], (0,199,190), thickness=2, lineType=cv2.LINE_AA)\n\n        return img_to_b64(img)\n\n    # ── Full inference ─────────────────────────────────────────────────────────\n\n    def run_full(self, img_bgr, fs=500, sig_len=5000):\n        \"\"\"\n        Returns:\n            steps      — list of 6 dicts {title, description, image(base64 JPEG)}\n            leads      — {lead_name: [float ...]} in mV, 500 Hz\n            lead_boxes — {lead_name: {x1, y1, x2, y2}} normalized on the Stage1 image\n            sampling_hz\n        \"\"\"\n        steps = []\n        img_rgb_orig = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n\n        steps.append({\n            'title':       'Raw Input Image',\n            'description': 'The original upload. May be rotated, unevenly lit, or photographed at an angle.',\n            'image':        img_to_b64(img_rgb_orig),\n        })\n        steps.append({\n            'title':       'Contrast Enhancement (CLAHE)',\n            'description': 'Adaptive histogram equalization on the HSV luminance channel sharpens the waveform traces without overexposing bright areas.',\n            'image':        img_to_b64(change_color(img_rgb_orig)),\n        })\n\n        img_pp_bgr = preprocess_by_source(img_bgr.copy(), source='0001')\n        steps.append({\n            'title':       'Image Preprocessing',\n            'description': 'Source-specific corrections: noise removal, white balance, illumination normalization.',\n            'image':        img_to_b64(cv2.cvtColor(img_pp_bgr, cv2.COLOR_BGR2RGB)),\n        })\n\n        # Run both preprocessing paths through Stage 0 + 1 and keep the better result\n        s0_raw, s0_pp = self._run_stage0(img_bgr), self._run_stage0(img_pp_bgr)\n        s1_raw, s1_pp = self._run_stage1(s0_raw),  self._run_stage1(s0_pp)\n        if stage1_quality(s1_pp) > stage1_quality(s1_raw) * 1.02:\n            s0_best, s1_best = s0_pp, s1_pp\n        else:\n            s0_best, s1_best = s0_raw, s1_raw\n\n        steps.append({\n            'title':       'Rotation Correction + Perspective Warp (Stage 0)',\n            'description': 'Keypoint network detects the four paper corners and applies a homography to produce a fronto-parallel view.',\n            'image':        img_to_b64(s0_best),\n        })\n        steps.append({\n            'title':       'ECG Grid Alignment (Stage 1)',\n            'description': 'Grid-point network corrects residual distortion in the mm-grid, making the pixel→mV conversion accurate.',\n            'image':        img_to_b64(s1_best),\n        })\n\n        series_4row, pixel_4ch = self._run_stage2(s1_best, length=sig_len)\n        steps.append({\n            'title':       'Waveform Segmentation Heatmap (Stage 2 · ResNet34-UNet)',\n            'description': 'Four-channel sigmoid output overlaid on the input. Orange-red regions are the pixels classified as ECG waveform.',\n            'image':        heatmap_overlay(s1_best, pixel_4ch),\n        })\n        steps.append({\n            'title':       'Extraction Overlay — Signal Reconstructed onto Source',\n            'description': 'The 12 extracted lead signals drawn back onto the Stage 1 image using the exact same calibration constants used during extraction. If the colored traces align with the printed waveforms, extraction is accurate.',\n            'image':        self._draw_overlay(s1_best, series_4row, pixel_4ch),\n        })\n\n        d = dw(series_dict(series_4row))\n        leads_out = {}\n        for lead in LEADS_ORDER:\n            arr = d.get('II', d.get('II_short', np.zeros(sig_len))) if lead == 'II'                   else d.get(lead, np.zeros(sig_len // 4))\n            leads_out[lead] = arr.tolist()\n\n        img_h, img_w   = s1_best.shape[:2]\n        lead_boxes     = self._compute_lead_boxes(img_h, img_w)\n\n        metrics = compute_metrics(leads_out['II'], fs)\n\n        return {\n            'steps':       steps,\n            'leads':       leads_out,\n            'lead_boxes':  lead_boxes,\n            'metrics':     metrics,\n            'sampling_hz': fs,\n        }\n\nprint('✅ PhysioPipeline defined')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 — Load models\n\nThis takes 1–2 minutes. The Stage 2 weight file is the largest (~350 MB) and uses a slightly non-standard checkpoint format, so we check for the `state_dict` key before loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pipeline = PhysioPipeline(device='cuda:0' if device == 'cuda' else 'cpu')\npipeline.load_models(STAGE0_W, STAGE1_W, STAGE2_W)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 — Smoke test *(optional)*\n\nDrop an ECG image into the Colab file panel (left sidebar), update the path, and run this cell to verify the full pipeline works end to end before starting the server.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "TEST_IMG = '/content/test_ecg.png'  # ← update path if needed\n\nif os.path.exists(TEST_IMG):\n    result = pipeline.run_full(cv2.imread(TEST_IMG))\n    print(f'steps:         {len(result[\"steps\"])}')\n    print(f'leads:         {list(result[\"leads\"].keys())}')\n    print(f'Lead II pts:   {len(result[\"leads\"][\"II\"])}')\n    print(f'lead_boxes[I]: {result[\"lead_boxes\"][\"I\"]}')\nelse:\n    print(f'⚠️  {TEST_IMG} not found — skipping (safe to proceed)')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 — Start the API server\n\nThe server is a minimal FastAPI app exposed over ngrok.\n\nA few things worth noting:\n- `nest_asyncio` patches the Colab event loop so `await server.serve()` works in a notebook cell\n- CORS is fully open (`allow_origins=['*']`) — fine for a demo, tighten for production\n- The ngrok URL changes on every Colab restart; paste the new one into your frontend each time\n\n**Replace `YOUR_NGROK_TOKEN_HERE`** with your token from [dashboard.ngrok.com/authtokens](https://dashboard.ngrok.com/authtokens).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\nos.environ['PYTORCH_ALLOC_CONF'] = 'expandable_segments:True'\n\nNGROK_TOKEN = 'YOUR_NGROK_TOKEN_HERE'  # ← paste your token here\n\nfrom pyngrok import ngrok, conf\nconf.get_default().auth_token = NGROK_TOKEN\n\nimport nest_asyncio, uvicorn\nfrom fastapi import FastAPI, UploadFile, File, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\n\nnest_asyncio.apply()\n\napp = FastAPI(title='ECG Digitizer')\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n\n@app.get('/')\ndef health():\n    return {'status': 'ok', 'device': device}\n\n@app.post('/digitize')\nasync def digitize(file: UploadFile = File(...), fs: int = 500, sig_len: int = 5000):\n    if not file.content_type.startswith('image/'):\n        raise HTTPException(400, detail='image files only')\n    contents = await file.read()\n    img_bgr  = cv2.imdecode(np.frombuffer(contents, np.uint8), cv2.IMREAD_COLOR)\n    if img_bgr is None:\n        raise HTTPException(422, detail='could not decode image')\n    try:\n        gc.collect()\n        torch.cuda.empty_cache()\n        result = pipeline.run_full(img_bgr, fs=fs, sig_len=sig_len)\n        gc.collect(); torch.cuda.empty_cache()\n        return JSONResponse(content=result)\n    except Exception as e:\n        import traceback; traceback.print_exc()\n        raise HTTPException(500, detail=str(e))\n\nPORT   = 8000\ntunnel = ngrok.connect(PORT, 'http')\nurl    = tunnel.public_url\n\nprint(f'  API   →  {url}/digitize')\nprint(f'  Docs  →  {url}/docs')\nprint('\\nKeep this cell running. The URL changes every time Colab restarts.')\n\nconfig = uvicorn.Config(app, host='0.0.0.0', port=PORT, log_level='warning')\nawait uvicorn.Server(config).serve()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## API reference\n\n### `POST /digitize`\n\n**Request** — `multipart/form-data`\n\n| field | type | description |\n|-------|------|-------------|\n| `file` | image file | JPEG, PNG, or any format OpenCV can decode |\n| `fs` | int (default 500) | target sampling rate in Hz |\n| `sig_len` | int (default 5000) | number of output samples |\n\n**Response** — JSON\n\n```json\n{\n  \"steps\": [\n    { \"title\": \"...\", \"description\": \"...\", \"image\": \"data:image/jpeg;base64,...\" },\n    ...\n  ],\n  \"leads\": {\n    \"I\":   [0.012, -0.003, ...],\n    \"II\":  [0.021,  0.008, ...],\n    ...\n  },\n  \"lead_boxes\": {\n    \"I\":   { \"x1\": 0.054, \"y1\": 0.264, \"x2\": 0.279, \"y2\": 0.498 },\n    ...\n  },\n  \"sampling_hz\": 500\n}\n```\n\n**`steps` index**\n\n| i | stage |\n|---|-------|\n| 0 | Raw input image |\n| 1 | CLAHE contrast enhancement |\n| 2 | Source-aware preprocessing |\n| 3 | Stage 0 — rotation + perspective warp |\n| 4 | Stage 1 — grid alignment |\n| 5 | Stage 2 — waveform segmentation heatmap |\n\n**`lead_boxes` coordinate system**\n\nAll values are normalized to `[0, 1]` relative to the **Stage 1 image** (`steps[4].image`).  \nTo get pixel coordinates for a rendered image of size `W × H`:\n\n```javascript\nconst { x1, y1, x2, y2 } = data.lead_boxes['I'];\nconst box = {\n  left:   x1 * W,\n  top:    y1 * H,\n  width:  (x2 - x1) * W,\n  height: (y2 - y1) * H,\n};\n```\n\n### Quick test with `curl`\n\n```bash\ncurl -X POST https://<your-ngrok-url>/digitize \\\n     -F \"file=@/path/to/ecg.jpg\" \\\n     | python3 -c \"import sys,json; d=json.load(sys.stdin); print(list(d['leads'].keys()))\"\n```\n"
   ]
  }
 ]
}